{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566},{"sourceId":5011320,"sourceType":"datasetVersion","datasetId":2907866}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-14T16:10:14.681283Z","iopub.execute_input":"2023-11-14T16:10:14.682177Z","iopub.status.idle":"2023-11-14T16:10:15.022520Z","shell.execute_reply.started":"2023-11-14T16:10:14.682143Z","shell.execute_reply":"2023-11-14T16:10:15.021561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport librosa\nimport torch\nimport librosa.display\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# to play the audio files\nfrom IPython.display import Audio\nplt.style.use('seaborn-white')","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:15.801425Z","iopub.execute_input":"2023-11-14T16:10:15.802589Z","iopub.status.idle":"2023-11-14T16:10:19.314991Z","shell.execute_reply.started":"2023-11-14T16:10:15.802549Z","shell.execute_reply":"2023-11-14T16:10:19.314015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install transformers","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:19.316653Z","iopub.execute_input":"2023-11-14T16:10:19.317069Z","iopub.status.idle":"2023-11-14T16:10:32.007989Z","shell.execute_reply.started":"2023-11-14T16:10:19.317042Z","shell.execute_reply":"2023-11-14T16:10:32.006820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install speechbrain","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:32.010192Z","iopub.execute_input":"2023-11-14T16:10:32.010518Z","iopub.status.idle":"2023-11-14T16:10:44.224010Z","shell.execute_reply.started":"2023-11-14T16:10:32.010491Z","shell.execute_reply":"2023-11-14T16:10:44.222904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"crema = \"/kaggle/input/cremad/AudioWAV/\"\ncrema_directory_list = os.listdir(crema)\nfile_name = []\nfile_emotion = []\nfile_path = []\nfile_id = []\n\nfor file in crema_directory_list:\n    # storing file paths\n    file_name.append(file.split('.')[0])\n    file_path.append(crema + file)\n    # storing file emotions\n    part=file.split('_')\n    if part[2] == 'SAD':\n        file_emotion.append('Sadness')\n    elif part[2] == 'ANG':\n        file_emotion.append('Anger')\n    elif part[2] == 'FEA':\n        file_emotion.append('Fear')\n    elif part[2] == 'HAP':\n        file_emotion.append('Happiness')\n    elif part[2] == 'NEU':\n        file_emotion.append('Neutral')\n    elif part[2] == 'DIS':\n        file_emotion.append('Disgust')\n    else:\n        file_emotion.append('Unknown')\n    \n    \n    file_id.append(int(part[0]))\n   \n\nfilename_df = pd.DataFrame(file_name, columns=['Name'])\npath_df = pd.DataFrame(file_path, columns=['Path'])\nemotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\nspeaker_id_df = pd.DataFrame(file_id, columns=['ActorID'])\n\nCrema_df = pd.concat([filename_df,path_df,emotion_df, speaker_id_df], axis=1)\nCrema_df['source'] = 'cremad'\ncdataset = Crema_df\ncemotions = ['Sadness', 'Happiness', 'Anger', 'Fear','Disgust', 'Neutral']\ncdataset = cdataset[cdataset['Emotions'].isin(cemotions)].reset_index(drop = True)\ncdataset['Emotions'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:44.225742Z","iopub.execute_input":"2023-11-14T16:10:44.226589Z","iopub.status.idle":"2023-11-14T16:10:44.608214Z","shell.execute_reply.started":"2023-11-14T16:10:44.226549Z","shell.execute_reply":"2023-11-14T16:10:44.607225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cdataset","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:44.610207Z","iopub.execute_input":"2023-11-14T16:10:44.610503Z","iopub.status.idle":"2023-11-14T16:10:44.629695Z","shell.execute_reply.started":"2023-11-14T16:10:44.610478Z","shell.execute_reply":"2023-11-14T16:10:44.628677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nresult = (\n  flights_df\n  .merge(airports_df, left_on='origin_airport', right_on='iata_code')\n)\n'''\ndemographis_df = pd.read_csv('/kaggle/input/cremad-demographics/VideoDemographics.csv')\n\ncdata=cdataset.merge(demographis_df, left_on='ActorID', right_on='ActorID')","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:44.630877Z","iopub.execute_input":"2023-11-14T16:10:44.631221Z","iopub.status.idle":"2023-11-14T16:10:44.663055Z","shell.execute_reply.started":"2023-11-14T16:10:44.631189Z","shell.execute_reply":"2023-11-14T16:10:44.662099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cdata.head\n","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:44.664179Z","iopub.execute_input":"2023-11-14T16:10:44.664521Z","iopub.status.idle":"2023-11-14T16:10:44.675384Z","shell.execute_reply.started":"2023-11-14T16:10:44.664489Z","shell.execute_reply":"2023-11-14T16:10:44.674432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cdata.drop(labels={'Race','Ethnicity'}, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:44.676496Z","iopub.execute_input":"2023-11-14T16:10:44.676778Z","iopub.status.idle":"2023-11-14T16:10:44.685117Z","shell.execute_reply.started":"2023-11-14T16:10:44.676734Z","shell.execute_reply":"2023-11-14T16:10:44.684395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cdata","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:44.686101Z","iopub.execute_input":"2023-11-14T16:10:44.686340Z","iopub.status.idle":"2023-11-14T16:10:44.705398Z","shell.execute_reply.started":"2023-11-14T16:10:44.686319Z","shell.execute_reply":"2023-11-14T16:10:44.704553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import preprocessing\nlec = preprocessing.LabelEncoder()\ncdataset['Emotion-labels'] = lec.fit_transform(cdataset['Emotions'])\nlec_name_mapping = dict(zip(lec.classes_, lec.transform(lec.classes_)))\nprint(lec_name_mapping)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:44.706308Z","iopub.execute_input":"2023-11-14T16:10:44.706572Z","iopub.status.idle":"2023-11-14T16:10:44.898876Z","shell.execute_reply.started":"2023-11-14T16:10:44.706549Z","shell.execute_reply":"2023-11-14T16:10:44.897982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CREMA_emotion_summary =cdata.pivot_table(index='Emotions',  aggfunc=len,  values='ActorID')\n\nCREMA_emotion_summary","metadata":{"execution":{"iopub.status.busy":"2023-11-14T13:39:45.564920Z","iopub.execute_input":"2023-11-14T13:39:45.565303Z","iopub.status.idle":"2023-11-14T13:39:45.583770Z","shell.execute_reply.started":"2023-11-14T13:39:45.565273Z","shell.execute_reply":"2023-11-14T13:39:45.582801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CREMA_gender_summary =cdata.pivot_table(index='Sex', aggfunc=len,  values='ActorID' )\n\nCREMA_gender_summary","metadata":{"execution":{"iopub.status.busy":"2023-11-14T13:39:47.137368Z","iopub.execute_input":"2023-11-14T13:39:47.137831Z","iopub.status.idle":"2023-11-14T13:39:47.153711Z","shell.execute_reply.started":"2023-11-14T13:39:47.137792Z","shell.execute_reply":"2023-11-14T13:39:47.152747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CREMA_age_summary =cdata.pivot_table(index='Age',  aggfunc=len,  values='ActorID')\n\nCREMA_age_summary","metadata":{"execution":{"iopub.status.busy":"2023-11-14T13:39:48.984685Z","iopub.execute_input":"2023-11-14T13:39:48.985741Z","iopub.status.idle":"2023-11-14T13:39:49.004894Z","shell.execute_reply.started":"2023-11-14T13:39:48.985691Z","shell.execute_reply":"2023-11-14T13:39:49.003813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CREMA_actor_summary =cdataset.pivot_table(index='ActorID',  aggfunc=len, values = 'source')\n\nCREMA_actor_summary","metadata":{"execution":{"iopub.status.busy":"2023-11-14T13:39:52.744178Z","iopub.execute_input":"2023-11-14T13:39:52.744922Z","iopub.status.idle":"2023-11-14T13:39:52.763238Z","shell.execute_reply.started":"2023-11-14T13:39:52.744877Z","shell.execute_reply":"2023-11-14T13:39:52.762295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ncdata['Sex'].value_counts().plot(kind=\"bar\", color=['blue', 'pink'])\n\n# Adding labels to the plot\nplt.xlabel('Gender')\nplt.ylabel('Count')\nplt.title('Distribution of Genders')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-14T13:41:07.895031Z","iopub.execute_input":"2023-11-14T13:41:07.895449Z","iopub.status.idle":"2023-11-14T13:41:08.156546Z","shell.execute_reply.started":"2023-11-14T13:41:07.895416Z","shell.execute_reply":"2023-11-14T13:41:08.155620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns  # Import Seaborn\n\n# Set Seaborn style\nsns.set(style='whitegrid')\n\n# Define a color palette\ncolors = sns.color_palette('pastel')\n\nplt.figure(figsize=(8, 6))\nsns.histplot(data=cdata, x='Age', palette=colors)\n\n# Adding labels to the plot\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Distribution of Ages')  # You might want to change the title\n\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:10:52.566402Z","iopub.execute_input":"2023-11-14T16:10:52.566796Z","iopub.status.idle":"2023-11-14T16:10:53.002749Z","shell.execute_reply.started":"2023-11-14T16:10:52.566746Z","shell.execute_reply":"2023-11-14T16:10:53.001797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# X vector","metadata":{}},{"cell_type":"code","source":"import torchaudio\nimport torch\n\n# Load the pre-trained x-vector model\nfrom speechbrain.pretrained import EncoderClassifier\nclassifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", savedir=\"pretrained_models/spkrec-xvect-voxceleb\", run_opts={\"device\":\"cuda\"} )\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclassifier.to(device)\n\ndef extract_xvector(path):\n    signal, fs =torchaudio.load(path)\n    embeddings = classifier.encode_batch(signal)\n    return np.array(embeddings.mean(axis = 0).cpu().squeeze())\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-11-14T13:49:33.317462Z","iopub.execute_input":"2023-11-14T13:49:33.317854Z","iopub.status.idle":"2023-11-14T13:49:38.697281Z","shell.execute_reply.started":"2023-11-14T13:49:33.317824Z","shell.execute_reply":"2023-11-14T13:49:38.696435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install tqdm","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:15:06.385272Z","iopub.execute_input":"2023-11-14T16:15:06.386014Z","iopub.status.idle":"2023-11-14T16:15:17.961665Z","shell.execute_reply.started":"2023-11-14T16:15:06.385978Z","shell.execute_reply":"2023-11-14T16:15:17.960487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:15:20.035786Z","iopub.execute_input":"2023-11-14T16:15:20.036187Z","iopub.status.idle":"2023-11-14T16:15:20.041152Z","shell.execute_reply.started":"2023-11-14T16:15:20.036153Z","shell.execute_reply":"2023-11-14T16:15:20.040266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xv_embeddings = []\nfor i in tqdm(range(len(cdata))):\n    features = extract_xvector(cdata['Path'][i])\n    xv_embeddings.append(features)\n\nxv_embeddings = np.array(xv_embeddings)\nprint(xv_embeddings.shape)\nxv_embedding = np.expand_dims(xv_embeddings, -1)\nprint(xv_embedding.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T13:55:55.345953Z","iopub.execute_input":"2023-11-14T13:55:55.346345Z","iopub.status.idle":"2023-11-14T13:57:30.927546Z","shell.execute_reply.started":"2023-11-14T13:55:55.346310Z","shell.execute_reply":"2023-11-14T13:57:30.926540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conformer","metadata":{}},{"cell_type":"code","source":"# #Trillson Reference: https://tfhub.dev/google/nonsemantic-speech-benchmark/trillsson4/1\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nm = hub.KerasLayer('https://tfhub.dev/google/trillsson4/1')\n\nimport torchaudio\ndef extract_trillson(path):\n    sample_rate = 16000\n    array, fs = torchaudio.load(path)\n    array = np.array(array)\n    # NOTE: Audio should be floats in [-1, 1], sampled at 16kHz. Model input is of\n    # the shape [batch size, time].\n    embeddings = m(array)['embedding']\n    # Models internally aggregate over time. For a time-series of embeddings, the\n    # user can frame audio however they want.\n    embeddings.shape.assert_is_compatible_with([None, 1024])\n    embeddings = np.squeeze(np.array(embeddings), axis = 0)\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2023-11-14T13:58:51.670910Z","iopub.execute_input":"2023-11-14T13:58:51.671285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\ntrillson_embeddings = []\nfor i in tqdm(range(len(cdata))):\n    features = extract_trillson(cdata['Path'][i])\n    trillson_embeddings.append(features)\n\ntrillson_embeddings = np.array(trillson_embeddings)\nprint(trillson_embeddings.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T13:59:53.287624Z","iopub.execute_input":"2023-11-14T13:59:53.288077Z","iopub.status.idle":"2023-11-14T14:10:15.272968Z","shell.execute_reply.started":"2023-11-14T13:59:53.288043Z","shell.execute_reply":"2023-11-14T14:10:15.271969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trillson_embeddings = np.expand_dims(trillson_embeddings, -1)\nprint(trillson_embeddings.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T14:12:14.367796Z","iopub.execute_input":"2023-11-14T14:12:14.368751Z","iopub.status.idle":"2023-11-14T14:12:14.373462Z","shell.execute_reply.started":"2023-11-14T14:12:14.368716Z","shell.execute_reply":"2023-11-14T14:12:14.372449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MFCC","metadata":{}},{"cell_type":"code","source":"#https://www.analyticsvidhya.com/blog/2022/03/implementing-audio-classification-project-using-deep-learning/\nfrom tqdm import tqdm\ndef extract_mfcc(path):\n    samplerate = 16000\n    audio, sample_rate = librosa.load(path, sr = samplerate)\n    #we extract mfcc\n    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n    #in order to find out scaled feature we do mean of transpose of value\n    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n    return mfccs_scaled_features\n\nmfcc_embeddings = []\nfor i in tqdm(range(len(cdata))):\n    features = extract_mfcc(cdata['Path'][i])\n    mfcc_embeddings.append(features)\n\nmfcc_embeddings = np.array(mfcc_embeddings)\nprint(mfcc_embeddings.shape)\n\nmfcc_embedding = np.expand_dims(mfcc_embeddings, -1)\nprint(mfcc_embedding.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T14:12:39.172667Z","iopub.execute_input":"2023-11-14T14:12:39.173157Z","iopub.status.idle":"2023-11-14T14:14:46.307320Z","shell.execute_reply.started":"2023-11-14T14:12:39.173121Z","shell.execute_reply":"2023-11-14T14:14:46.305990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XLSR","metadata":{}},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoProcessor, AutoModel, Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2FeatureExtractor\n\nprocessor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-1b\")\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-xls-r-1b\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)\nimport torchaudio\n\ndef extract_xlsr(path):\n    sample_rate = 16000\n    array, fs = torchaudio.load(path)\n    input = processor(array.squeeze(), sampling_rate= sample_rate, return_tensors=\"pt\").to(device)\n    # apply the model to the input array from wav\n    with torch.no_grad():\n       outputs = model(**input)\n    # extract last hidden state, compute average, convert to numpy\n    last_hidden_states = outputs.last_hidden_state.squeeze().mean(axis=0).cpu().numpy()\n    return last_hidden_states\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:24:29.226834Z","iopub.execute_input":"2023-11-14T16:24:29.227887Z","iopub.status.idle":"2023-11-14T16:24:39.584561Z","shell.execute_reply.started":"2023-11-14T16:24:29.227842Z","shell.execute_reply":"2023-11-14T16:24:39.583698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi --query-gpu=memory.used --format=csv -l","metadata":{"execution":{"iopub.status.busy":"2023-11-14T14:31:27.728561Z","iopub.execute_input":"2023-11-14T14:31:27.729439Z","iopub.status.idle":"2023-11-14T14:31:32.258096Z","shell.execute_reply.started":"2023-11-14T14:31:27.729403Z","shell.execute_reply":"2023-11-14T14:31:32.256907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xlsr_embeddings = []\nfor i in tqdm(range(len(cdata))):\n    features = extract_xlsr(cdata['Path'][i])\n    xlsr_embeddings.append(features)\n    \n\nxlsr_embeddings = np.array(xlsr_embeddings)\nprint(xlsr_embeddings.shape)\nxlsr_embedding = np.expand_dims(xlsr_embeddings, -1)\nprint(xlsr_embedding.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:24:53.394553Z","iopub.execute_input":"2023-11-14T16:24:53.395011Z","iopub.status.idle":"2023-11-14T16:36:20.538618Z","shell.execute_reply.started":"2023-11-14T16:24:53.394973Z","shell.execute_reply":"2023-11-14T16:36:20.537655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('/kaggle/working/CREMAD_xv_embeddings.npy', xv_embedding)\nnp.save('/kaggle/working/CREMAD_mfcc_embeddings.npy', mfcc_embedding)\nnp.save('/kaggle/working/CREMAD_trillsson_embeddings.npy', trillson_embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T14:25:15.627967Z","iopub.execute_input":"2023-11-14T14:25:15.628644Z","iopub.status.idle":"2023-11-14T14:25:15.670741Z","shell.execute_reply.started":"2023-11-14T14:25:15.628612Z","shell.execute_reply":"2023-11-14T14:25:15.669659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('/kaggle/working/xlsr_embeddings.npy', xlsr_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:40:24.947322Z","iopub.execute_input":"2023-11-14T16:40:24.947695Z","iopub.status.idle":"2023-11-14T16:40:24.979983Z","shell.execute_reply.started":"2023-11-14T16:40:24.947665Z","shell.execute_reply":"2023-11-14T16:40:24.979183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HuBERT","metadata":{}},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoProcessor, AutoModel, Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2FeatureExtractor\n\nprocessor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/hubert-base-ls960\")\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/hubert-base-ls960\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)\n\nimport torchaudio\n\ndef extract_Hubert(path):\n    sample_rate = 16000\n    array, fs = torchaudio.load(path)\n    input = processor(array.squeeze(), sampling_rate= sample_rate, return_tensors=\"pt\").to(device)\n    # apply the model to the input array from wav\n    with torch.no_grad():\n       outputs = model(**input)\n    # extract last hidden state, compute average, convert to numpy\n    last_hidden_states = outputs.last_hidden_state.squeeze().mean(axis=0).cpu().numpy()\n    return last_hidden_states\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:51:25.906459Z","iopub.execute_input":"2023-11-14T16:51:25.907488Z","iopub.status.idle":"2023-11-14T16:51:29.099292Z","shell.execute_reply.started":"2023-11-14T16:51:25.907453Z","shell.execute_reply":"2023-11-14T16:51:29.098479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hubert_embeddings = []\nfor i in tqdm(range(len(cdata))):\n    features = extract_Hubert(cdata['Path'][i])\n    hubert_embeddings.append(features)\n    \n\nhubert_embeddings = np.array(hubert_embeddings)\nprint(hubert_embeddings.shape)\nhubert_embedding = np.expand_dims(hubert_embeddings, -1)\nprint(hubert_embedding.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:52:21.626843Z","iopub.execute_input":"2023-11-14T16:52:21.627791Z","iopub.status.idle":"2023-11-14T16:54:27.724719Z","shell.execute_reply.started":"2023-11-14T16:52:21.627727Z","shell.execute_reply":"2023-11-14T16:54:27.723710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('/kaggle/working/hubert_embeddings.npy', hubert_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:54:41.198746Z","iopub.execute_input":"2023-11-14T16:54:41.199217Z","iopub.status.idle":"2023-11-14T16:54:41.225587Z","shell.execute_reply.started":"2023-11-14T16:54:41.199184Z","shell.execute_reply":"2023-11-14T16:54:41.224627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# W2V2 SAT","metadata":{}},{"cell_type":"code","source":"# Reference; https://bagustris.wordpress.com/2022/08/23/acoustic-feature-extraction-with-transformers/\n\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model\nimport torchaudio\n\n# load model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\nwavtwovectwo = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n\n\n\n\n\n# audio file is decoded on the fly\n\ndef extract_w2v2(path):\n    sample_rate = 16000\n    array, fs = torchaudio.load(path)\n    array  = np.array(array)\n    array = np.mean(array, axis = 0)\n    input = processor(array.squeeze(), sampling_rate= sample_rate, return_tensors=\"pt\").to(device)\n    # apply the model to the input array from wav\n    with torch.no_grad():\n        outputs = wavtwovectwo(**input)\n    # extract last hidden state, compute average, convert to numpy\n    last_hidden_states = outputs.last_hidden_state.squeeze().mean(axis=0).to(\"cpu\").numpy()\n    return last_hidden_states\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:56:35.357495Z","iopub.execute_input":"2023-11-14T16:56:35.357910Z","iopub.status.idle":"2023-11-14T16:56:36.761147Z","shell.execute_reply.started":"2023-11-14T16:56:35.357874Z","shell.execute_reply":"2023-11-14T16:56:36.760344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v2_embeddings = []\nfor i in tqdm(range(len(cdata))):\n    features = extract_w2v2(cdata['Path'][i])\n    w2v2_embeddings.append(features)\n    \n\nw2v2_embeddings = np.array(w2v2_embeddings)\nprint(w2v2_embeddings.shape)\nw2v2_embedding = np.expand_dims(w2v2_embeddings, -1)\nprint(w2v2_embedding.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:56:42.966404Z","iopub.execute_input":"2023-11-14T16:56:42.966783Z","iopub.status.idle":"2023-11-14T16:58:41.677380Z","shell.execute_reply.started":"2023-11-14T16:56:42.966741Z","shell.execute_reply":"2023-11-14T16:58:41.676388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('/kaggle/working/w2v2_embeddings.npy', w2v2_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T16:59:08.191399Z","iopub.execute_input":"2023-11-14T16:59:08.191839Z","iopub.status.idle":"2023-11-14T16:59:08.214149Z","shell.execute_reply.started":"2023-11-14T16:59:08.191806Z","shell.execute_reply":"2023-11-14T16:59:08.213335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Unispeech","metadata":{}},{"cell_type":"code","source":"from transformers import AutoProcessor, UniSpeechSatModel, Wav2Vec2FeatureExtractor\nimport torchaudio\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprocessor = Wav2Vec2FeatureExtractor.from_pretrained(\"microsoft/unispeech-sat-base\")\nunispeechsat = UniSpeechSatModel.from_pretrained(\"microsoft/unispeech-sat-base\")\nunispeechsat.to(device)\ndef extract_unispeech(path):\n    sample_rate = 16000\n    array, fs = torchaudio.load(path)\n    array = np.array(array)\n    array = np.mean(array, axis = 0)\n    input = processor(array.squeeze(), sampling_rate=sample_rate, return_tensors=\"pt\")\n    input = input.to(device)\n    with torch.no_grad():\n       outputs = unispeechsat(**input)\n    last_hidden_states = outputs.last_hidden_state.squeeze().mean(axis=0).to(\"cpu\").numpy()\n    #embeddings = torch.nn.functional.normalize(features, dim=-1).cpu()\n    return last_hidden_states\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-11-14T17:01:32.410431Z","iopub.execute_input":"2023-11-14T17:01:32.410822Z","iopub.status.idle":"2023-11-14T17:01:34.153966Z","shell.execute_reply.started":"2023-11-14T17:01:32.410789Z","shell.execute_reply":"2023-11-14T17:01:34.153160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unispeech_embeddings = []\nfor i in tqdm(range(len(cdata))):\n    features = extract_unispeech(cdata['Path'][i])\n    unispeech_embeddings.append(features)\n    \n\nunispeech_embeddings = np.array(unispeech_embeddings)\nprint(unispeech_embeddings.shape)\nunispeech_embedding = np.expand_dims(unispeech_embeddings, -1)\nprint(unispeech_embedding.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T17:01:36.280994Z","iopub.execute_input":"2023-11-14T17:01:36.281360Z","iopub.status.idle":"2023-11-14T17:03:35.845373Z","shell.execute_reply.started":"2023-11-14T17:01:36.281331Z","shell.execute_reply":"2023-11-14T17:03:35.844431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('/kaggle/working/unispeech_embeddings.npy', unispeech_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T17:03:39.856318Z","iopub.execute_input":"2023-11-14T17:03:39.857247Z","iopub.status.idle":"2023-11-14T17:03:39.879246Z","shell.execute_reply.started":"2023-11-14T17:03:39.857212Z","shell.execute_reply":"2023-11-14T17:03:39.878438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WavLM","metadata":{}},{"cell_type":"code","source":"from transformers import AutoProcessor, WavLMModel, Wav2Vec2Processor \nimport torchaudio\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprocessor = Wav2Vec2Processor.from_pretrained(\"patrickvonplaten/wavlm-libri-clean-100h-base-plus\")\nwavlm = WavLMModel.from_pretrained(\"patrickvonplaten/wavlm-libri-clean-100h-base-plus\")# audio file is decoded on the fly\nwavlm.to(device)\n\ndef extract_wavlm(path):\n    sample_rate = 16000\n    array, fs = torchaudio.load(path)\n    array  = np.array(array)\n    array = np.mean(array, axis = 0)\n    input = processor(array.squeeze(), sampling_rate= sample_rate, return_tensors=\"pt\").to(device)\n    # apply the model to the input array from wav\n    input = input.to(device)\n    with torch.no_grad():\n       outputs = wavlm(**input)\n    # extract last hidden state, compute average, convert to numpy\n    last_hidden_states = outputs.last_hidden_state.squeeze().mean(axis=0).to(\"cpu\").numpy()\n    return last_hidden_states\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-11-14T17:06:12.573148Z","iopub.execute_input":"2023-11-14T17:06:12.573610Z","iopub.status.idle":"2023-11-14T17:06:26.945414Z","shell.execute_reply.started":"2023-11-14T17:06:12.573578Z","shell.execute_reply":"2023-11-14T17:06:26.944233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wavlm_embeddings = []\nfor i in tqdm(range(len(cdata))):\n    features = extract_wavlm(cdata['Path'][i])\n    wavlm_embeddings.append(features)\n    \n\nwavlm_embeddings = np.array(wavlm_embeddings)\nprint(wavlm_embeddings.shape)\nwavlm_embedding = np.expand_dims(wavlm_embeddings, -1)\nprint(wavlm_embedding.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T17:06:26.947142Z","iopub.execute_input":"2023-11-14T17:06:26.947441Z","iopub.status.idle":"2023-11-14T17:09:12.185345Z","shell.execute_reply.started":"2023-11-14T17:06:26.947415Z","shell.execute_reply":"2023-11-14T17:09:12.184367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('/kaggle/working/wavlm_embeddings.npy', wavlm_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T17:18:50.911478Z","iopub.execute_input":"2023-11-14T17:18:50.912478Z","iopub.status.idle":"2023-11-14T17:18:50.935182Z","shell.execute_reply.started":"2023-11-14T17:18:50.912440Z","shell.execute_reply":"2023-11-14T17:18:50.934090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MMS","metadata":{}},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoModelForPreTraining, Wav2Vec2FeatureExtractor\nimport torchaudio\nfrom transformers import Wav2Vec2Processor, Wav2Vec2Model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprocessor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/mms-1b\")\nmodel = Wav2Vec2Model.from_pretrained(\"facebook/mms-1b\")\nmodel.to(device)\n\ndef extract_mms(path):\n    sample_rate = 16000\n    array, fs = torchaudio.load(path)\n    array  = np.array(array)\n    array = np.mean(array, axis = 0)\n    input = processor(array.squeeze(), sampling_rate=sample_rate, return_tensors=\"pt\").to(device)\n    input = input.to(device)\n    with torch.no_grad():\n       outputs = model(**input)\n    last_hidden_states = outputs.last_hidden_state.squeeze().mean(axis=0).to(\"cpu\").numpy()\n    #embeddings = torch.nn.functional.normalize(features, dim=-1).cpu()\n    return last_hidden_states\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2023-11-14T17:26:15.252826Z","iopub.execute_input":"2023-11-14T17:26:15.253222Z","iopub.status.idle":"2023-11-14T17:26:25.288926Z","shell.execute_reply.started":"2023-11-14T17:26:15.253189Z","shell.execute_reply":"2023-11-14T17:26:25.287958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mms_embeddings = []\nfor i in tqdm(range(len(cdata))):\n    features = extract_mms(cdata['Path'][i])\n    mms_embeddings.append(features)\n    \n\nmms_embeddings = np.array(mms_embeddings)\nprint(mms_embeddings.shape)\nmms_embedding = np.expand_dims(mms_embeddings, -1)\nprint(mms_embedding.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T17:26:53.474944Z","iopub.execute_input":"2023-11-14T17:26:53.475953Z","iopub.status.idle":"2023-11-14T17:37:06.480633Z","shell.execute_reply.started":"2023-11-14T17:26:53.475918Z","shell.execute_reply":"2023-11-14T17:37:06.479562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('/kaggle/working/CREMAD_mms_embeddings.npy', mms_embedding)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T17:38:28.191565Z","iopub.execute_input":"2023-11-14T17:38:28.192321Z","iopub.status.idle":"2023-11-14T17:38:28.226585Z","shell.execute_reply.started":"2023-11-14T17:38:28.192288Z","shell.execute_reply":"2023-11-14T17:38:28.225655Z"},"trusted":true},"execution_count":null,"outputs":[]}]}